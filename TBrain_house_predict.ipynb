{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model,svm,gaussian_process\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,BaggingRegressor,GradientBoostingRegressor,ExtraTreesRegressor\n",
    "from sklearn.model_selection import KFold,cross_val_score,train_test_split,StratifiedKFold\n",
    "from sklearn.linear_model import ElasticNet, Lasso, Ridge\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.linear_model import ElasticNet, SGDRegressor, BayesianRidge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from xgboost import XGBRegressor#XGB迴歸也來一組參數\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from scipy.stats import skew\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.preprocessing import RobustScaler,StandardScaler\n",
    "%matplotlib inline\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "data = pd.read_csv('C:\\\\Users\\\\kikok\\\\Downloads\\\\dataset\\\\train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata[\"oTotal_floor\"] = data.total_floor.map({2:1,8:1,18:1, \\n                                         16:2, 3:2, 4:2,5:2,9:2,10:2,\\n                                         6:3,12:3 ,13:3, 19:3,20:3,21:3, \\n                                         1:4,7:4 ,14:4, 15:4,17:4,24:4,29:4,  \\n                                         11:5,23:5 ,22:5, 25:5,  \\n                                         26:6,27:6 ,28:6})\\ndata[\"oTxn_floor\"] = data.txn_floor.map({2:1,3:1,4:1,5:1, \\n                                         26:2, 16:2, 14:2,0:2,6:2,8:2,\\n                                         1:3,7:3 ,9:3, 10:3,11:3,12:3, \\n                                         13:4,21:4 ,22:4, 15:4,17:4,  \\n                                         18:5,19:5 ,20:5, 24:5,  \\n                                         23:6,25:6,27:6 ,28:6})\\ndata[\"oCity\"] = data.city.map({7:1,9:1, \\n                               3:2, 5:2, 13:2,17:2,\\n                               6:3,10:3 ,14:3, 21:3,\\n                               12:4})\\ndata =data.drop([\\'txn_floor\\',\\'city\\',\\'total_floor\\'],axis=1)\\n\\ndata[cols2] = np.log1p(data[cols2])\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cols=['building_area','land_area','building_complete_dt'\n",
    "      ,'txn_dt','village_income_median','lon','total_floor','lat',\n",
    "      'txn_floor','XIV_10000'\n",
    "      ,'town','XIV_5000','parking_price','XIII_10000',\n",
    "      'town_area','town_population_density','XIII_5000','XIV_1000','village','XIII_MIN']\n",
    "cols2=['building_area','land_area','parking_price','village_income_median']\n",
    "cols3=['parking_area','parking_price','txn_floor']\n",
    "'''   \n",
    "cols_rfr=['building_area','XIII_10000','building_complete_dt'\n",
    "      ,'land_area','jobschool_rate','XIII_5000','txn_dt','village_income_median',\n",
    "      'V_10000','elementary_rate'\n",
    "      ,'txn_floor','junior_rate','divorce_rate','highschool_rate',\n",
    "      'VII_1000','V_5000','master_rate','XIV_5000','marriage_rate','bachelor_rate']\n",
    "cols=['building_area','highschool_rate','elementary_rate'\n",
    "      ,'master_rate','bachelor_rate','lon','village_income_median','XIII_5000',\n",
    "      'jobschool_rate','X_10000'\n",
    "      ,'II_10000','XII_10000','III_1000','XIII_10000',\n",
    "      'IX_10000','VII_10000','VIII_10000','XI_10000','V_10000','VI_10000']\n",
    "'''\n",
    "for col in cols3:\n",
    "    data[col].fillna(0, inplace=True)\n",
    "data['village_income_median'].fillna(data['village_income_median'].median(), inplace=True)    \n",
    "'''\n",
    "data[\"oTotal_floor\"] = data.total_floor.map({2:1,8:1,18:1, \n",
    "                                         16:2, 3:2, 4:2,5:2,9:2,10:2,\n",
    "                                         6:3,12:3 ,13:3, 19:3,20:3,21:3, \n",
    "                                         1:4,7:4 ,14:4, 15:4,17:4,24:4,29:4,  \n",
    "                                         11:5,23:5 ,22:5, 25:5,  \n",
    "                                         26:6,27:6 ,28:6})\n",
    "data[\"oTxn_floor\"] = data.txn_floor.map({2:1,3:1,4:1,5:1, \n",
    "                                         26:2, 16:2, 14:2,0:2,6:2,8:2,\n",
    "                                         1:3,7:3 ,9:3, 10:3,11:3,12:3, \n",
    "                                         13:4,21:4 ,22:4, 15:4,17:4,  \n",
    "                                         18:5,19:5 ,20:5, 24:5,  \n",
    "                                         23:6,25:6,27:6 ,28:6})\n",
    "data[\"oCity\"] = data.city.map({7:1,9:1, \n",
    "                               3:2, 5:2, 13:2,17:2,\n",
    "                               6:3,10:3 ,14:3, 21:3,\n",
    "                               12:4})\n",
    "data =data.drop(['txn_floor','city','total_floor'],axis=1)\n",
    "\n",
    "data[cols2] = np.log1p(data[cols2])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "data2 =data.drop(['building_id','total_price'],axis=1)\n",
    "x = data2\n",
    "y = np.log1p(data['total_price'])\n",
    "X_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.33,random_state = 42)\n",
    "X_train_scaled = scaler.fit(x).transform(X_train)\n",
    "X_test_scaled = scaler.fit(x).transform(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('feature_selection', SelectFromModel(estimator=XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, max_features=0.3, min_child_weight=1, missing=None,\n",
       "       n_estimators=500, n_jobs=...\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "  ('feature_selection', SelectFromModel(XGBRegressor(n_estimators=500, max_features=0.3))),\n",
    "  ('predict', XGBRegressor(max_depth=20)),\n",
    "])\n",
    "clf.fit(X_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb: 0.962840\n"
     ]
    }
   ],
   "source": [
    "#clf1=XGBRegressor(n_estimators=10000,max_depth=20,min_child_weight=2,gamma=0.1)\n",
    "clf.fit(X_train_scaled,y_train)\n",
    "#print(\"{}: {:.6f}\".format(\"xgb\",xgb.score(X_test_scaled,y_test)))\n",
    "print(\"{}: {:.6f}\".format(\"xgb\",clf.score(X_test_scaled,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('C:\\\\Users\\\\kikok\\\\Downloads\\\\dataset\\\\test.csv')\n",
    "for col in cols3:\n",
    "    test_data[col].fillna(0, inplace=True)\n",
    "test_data['village_income_median'].fillna(test_data['village_income_median'].median(), inplace=True)    \n",
    "'''\n",
    "test_data[\"oTotal_floor\"] = test_data.total_floor.map({2:1,8:1,18:1, \n",
    "                                         16:2, 3:2, 4:2,5:2,9:2,10:2,\n",
    "                                         6:3,12:3 ,13:3, 19:3,20:3,21:3, \n",
    "                                         1:4,7:4 ,14:4, 15:4,17:4,24:4,29:4,  \n",
    "                                         11:5,23:5 ,22:5, 25:5,  \n",
    "                                         26:6,27:6 ,28:6})\n",
    "test_data[\"oTxn_floor\"] = test_data.txn_floor.map({2:1,3:1,4:1,5:1, \n",
    "                                         26:2, 16:2, 14:2,0:2,6:2,8:2,\n",
    "                                         1:3,7:3 ,9:3, 10:3,11:3,12:3, \n",
    "                                         13:4,21:4 ,22:4, 15:4,17:4,  \n",
    "                                         18:5,19:5 ,20:5, 24:5,  \n",
    "                                         23:6,25:6,27:6 ,28:6})\n",
    "test_data[\"oCity\"] = test_data.city.map({7:1,9:1, \n",
    "                               3:2, 5:2, 13:2,17:2,\n",
    "                               6:3,10:3 ,14:3, 21:3,\n",
    "                               12:4})\n",
    "test_data2 =test_data.drop(['txn_floor','city','total_floor','building_id'],axis=1)\n",
    "'''\n",
    "test_data2 = test_data.drop(['building_id'],axis=1)\n",
    "test_data2[cols2] = np.log1p(test_data2[cols2])\n",
    "test_x_predict = test_data2.values\n",
    "test_x_predict_scaled = scaler.fit(test_x_predict).transform(test_x_predict)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y_ridge = rigit.predict(x_test_data)\n",
    "#y_rf = np.expm1(clf.predict(test_x_predict_scaled))\n",
    "#y_bagging = bag.predict(x_test_data)\n",
    "#y_adaboost = ada.predict(x_test_data)\n",
    "y_xgb =  np.expm1(clf.predict(test_x_predict_scaled))\n",
    "#y_Extra =  np.expm1(Extra.predict(test_x_predict_scaled))\n",
    "#y_stack = np.expm1(stack_model.predict(test_x_predict_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5805021.5  ,  4035395.75 ,  7430225.   , ...,  1281889.625,\n",
       "        2359612.5  ,  2794602.25 ], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = pd.DataFrame(y_xgb,columns=['total_price'])\n",
    "result = pd.concat((test_data['building_id'],prediction),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "result.to_csv('C:\\\\Users\\\\kikok\\\\Downloads\\\\dataset\\\\submit_test_20190514__0.962840_xgb.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class stacking(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self,mod,meta_model):\n",
    "        self.mod = mod\n",
    "        self.meta_model = meta_model\n",
    "        self.kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        self.saved_model = [list() for i in self.mod]\n",
    "        oof_train = np.zeros((X.shape[0], len(self.mod)))\n",
    "        \n",
    "        for i,model in enumerate(self.mod):\n",
    "            for train_index, val_index in self.kf.split(X,y):\n",
    "                renew_model = clone(model)\n",
    "                renew_model.fit(X[train_index], y[train_index])\n",
    "                self.saved_model[i].append(renew_model)\n",
    "                oof_train[val_index,i] = renew_model.predict(X[val_index])\n",
    "        \n",
    "        self.meta_model.fit(oof_train,y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self,X):\n",
    "        whole_test = np.column_stack([np.column_stack(model.predict(X) for model in single_model).mean(axis=1) \n",
    "                                      for single_model in self.saved_model]) \n",
    "        return self.meta_model.predict(whole_test)\n",
    "    \n",
    "    def get_oof(self,X,y,test_X):\n",
    "        oof = np.zeros((X.shape[0],len(self.mod)))\n",
    "        test_single = np.zeros((test_X.shape[0],5))\n",
    "        test_mean = np.zeros((test_X.shape[0],len(self.mod)))\n",
    "        for i,model in enumerate(self.mod):\n",
    "            for j, (train_index,val_index) in enumerate(self.kf.split(X,y)):\n",
    "                clone_model = clone(model)\n",
    "                clone_model.fit(X[train_index],y[train_index])\n",
    "                oof[val_index,i] = clone_model.predict(X[val_index])\n",
    "                test_single[:,j] = clone_model.predict(test_X)\n",
    "            test_mean[:,i] = test_single.mean(axis=1)\n",
    "        return oof, test_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "a = Imputer().fit_transform(X_train_scaled)\n",
    "b = Imputer().fit_transform(y_train.values.reshape(-1,1)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse_cv(model,X,y):\n",
    "    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=5))\n",
    "    return rmse\n",
    "\n",
    "lasso = Lasso(alpha=0.0005,max_iter=10000)\n",
    "ridge = Ridge(alpha=60)\n",
    "svr = SVR(gamma= 0.0004,kernel='rbf',C=13,epsilon=0.009)\n",
    "ker = KernelRidge(alpha=0.2 ,kernel='polynomial',degree=3 , coef0=0.8)\n",
    "ela = ElasticNet(alpha=0.005,l1_ratio=0.08,max_iter=10000)\n",
    "bay = BayesianRidge()\n",
    "xgb =  XGBRegressor(max_depth=20)\n",
    "rfr = RandomForestRegressor(n_estimators=500, max_features=0.3)\n",
    "\n",
    "stack_model = stacking(mod=[rfr,ela,lasso,ridge],meta_model=xgb)\n",
    "score = rmse_cv(stack_model,a,b)\n",
    "print(score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stack_model.fit(a,b)\n",
    "print(\"{}: {:.6f}\".format(\"stack_model\",stack_model.score(X_test_scaled,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse_cv(model,X,y):\n",
    "    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=5))\n",
    "    return rmse\n",
    "models = [Ridge(),Lasso(alpha=0.01,max_iter=10000),RandomForestRegressor(),GradientBoostingRegressor(),SVR(),LinearSVR(),\n",
    "          ElasticNet(alpha=0.001,max_iter=10000),SGDRegressor(max_iter=1000,tol=1e-3),BayesianRidge(),KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5),\n",
    "          ExtraTreesRegressor(),XGBRegressor()]\n",
    "\n",
    "names = [ \"Ridge\", \"Lasso\", \"RF\", \"GBR\", \"SVR\", \"LinSVR\", \"Ela\",\"SGD\",\"Bay\",\"Ker\",\"Extra\",\"Xgb\"]\n",
    "for name, model in zip(names, models):\n",
    "    score = rmse_cv(model, X_train_scaled, y_train)\n",
    "    print(\"{}: {:.6f}, {:.4f}\".format(name,score.mean(),score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y_final = (y_rf + y_xgb) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = [.1, .3, .5, .7, .9, .99]\n",
    "test_scores = []\n",
    "for max_feat in max_features:\n",
    "    clf = RandomForestRegressor(n_estimators=200, max_features=max_feat)\n",
    "    test_score = np.sqrt(-cross_val_score(clf, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error'))\n",
    "    test_scores.append(np.mean(test_score))\n",
    "plt.plot(max_features, test_scores)\n",
    "plt.title(\"Max Features vs CV Error\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphas_alt = [-0.01,0.001,0.005,0.01,0.02,0.05,1,1.5]\n",
    "kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "test_scores = []\n",
    "for alpha in alphas_alt:\n",
    "    clf = Ridge(alpha)\n",
    "    test_score = np.sqrt(-cross_val_score(clf, X_train_scaled, y_train, cv=kfolds, scoring='neg_mean_squared_error'))\n",
    "    test_scores.append(np.mean(test_score))\n",
    "plt.plot(alphas_alt, test_scores)\n",
    "plt.title(\"alphas_alt vs CV Error\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rigit = Ridge(1)\n",
    "params = [1,2,3,4,5,6,10,20]\n",
    "test_scores = []\n",
    "for param in params:\n",
    "    clf = XGBRegressor(max_depth=param,base_estimator=rigit)\n",
    "    test_score = np.sqrt(-cross_val_score(clf, X_train_scaled, y_train, cv=kfolds,scoring='neg_mean_squared_error'))\n",
    "    test_scores.append(np.mean(test_score))\n",
    "plt.plot(params, test_scores)\n",
    "plt.title(\"param vs CV Error\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rigit = Ridge(1)\n",
    "xgb =XGBRegressor(max_depth=20,base_estimator=rigit)\n",
    "xgb.fit( X_train_scaled, y_train)\n",
    "rfr = RandomForestRegressor(n_estimators=500, max_features=0.3)\n",
    "rfr.fit( X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class skew_dummies(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,skew=0.5):\n",
    "        self.skew = skew\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        X_numeric=X.select_dtypes(exclude=[\"object\"])\n",
    "        skewness = X_numeric.apply(lambda x: skew(x))\n",
    "        skewness_features = skewness[abs(skewness) >= self.skew].index\n",
    "        X[skewness_features] = np.log1p(X[skewness_features])\n",
    "        X = pd.get_dummies(X)\n",
    "        return X\n",
    "pipe = Pipeline([('skew_dummies', skew_dummies(skew=1)),])    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "data2 =data.drop(['lat'],axis=1)\n",
    "data_pipe = pipe.fit_transform(data2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
